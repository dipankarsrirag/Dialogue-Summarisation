{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import json\n",
    "\n",
    "\n",
    "def combine_columns(row):\n",
    "    try:\n",
    "        return row['dialogue'] + ' ' + row['summary']\n",
    "    except RuntimeError:\n",
    "        print(\"PandasDataFrameNotFound\")\n",
    "\n",
    "\n",
    "def fix_contractions(text):\n",
    "    with open('../data/contractions.json', 'r') as f:\n",
    "        contractions = json.load(f)\n",
    "    tokens = text.split()\n",
    "    cleaned = []\n",
    "    for token in tokens:\n",
    "        cleaned.append(contractions.get(token, token))\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    text = fix_contractions(text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    text = ' '.join(tokens).lower()\n",
    "    text = text.replace('# person1 #', '#person1#')\n",
    "    text = text.replace('# person2 #', '#person2#')\n",
    "    text = text.replace('# person3 #', '#person3#')\n",
    "    text = text.replace('# person4 #', '#person4#')\n",
    "    text = text.replace('# person5 #', '#person5#')\n",
    "    text = text.replace('# person6 #', '#person6#')\n",
    "    text = text.replace('# person7 #', '#person7#')\n",
    "    text = text.replace(' ,', ',')\n",
    "    text = text.replace(' .', '.')\n",
    "    text = text.replace(' ?', '?')\n",
    "    text = text.replace(' !', '!')\n",
    "    text = text.replace(\" ' \", \"'\")\n",
    "    text = text.replace(\"< \", \"<\")\n",
    "    text = text.replace(\" >\", \">\")\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train = pd.read_json(\n",
    "        '../data/raw/dialogsum/dialogsum.train.jsonl', lines=True)\n",
    "    train['summary'] = train['summary'].apply(\n",
    "        lambda x: '<SOS> ' + x + ' <EOS>')\n",
    "\n",
    "    corpus = train.apply(combine_columns, axis=1)\n",
    "    corpus = list(map(tokenize, corpus))\n",
    "    corpus = '\\n'.join(corpus)\n",
    "\n",
    "    with open(\"../data/processed/corpus\", \"w\") as f:\n",
    "        f.write(corpus)\n",
    "\n",
    "    dev = pd.read_json(\n",
    "        '../data/raw/dialogsum/dialogsum.dev.jsonl', lines=True)\n",
    "    dev['summary'] = dev['summary'].apply(\n",
    "        lambda x: '<SOS> ' + x + ' <EOS>')\n",
    "\n",
    "    dev_corpus = dev.apply(combine_columns, axis=1)\n",
    "    dev_corpus = list(map(tokenize, corpus))\n",
    "    dev_corpus = '\\n'.join(corpus)\n",
    "\n",
    "    test = pd.read_json(\n",
    "        '../data/raw/dialogsum/dialogsum.test.jsonl', lines=True)[['dialogue', 'summary1']]\n",
    "    test.columns = ['dialogue', 'summary']\n",
    "    test['summary'] = test['summary'].apply(\n",
    "        lambda x: '<SOS> ' + x + ' <EOS>')\n",
    "\n",
    "    test_corpus = test.apply(combine_columns, axis=1)\n",
    "    test_corpus = list(map(tokenize, corpus))\n",
    "    test_corpus = '\\n'.join(corpus)\n",
    "\n",
    "    corpus.extend(dev_corpus)\n",
    "    corpus.extend(test_corpus)\n",
    "\n",
    "    with open(\"../data/processed/glove_corpus\", \"w\") as f:\n",
    "        f.write(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
